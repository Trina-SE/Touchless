{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# EMG Gesture Recognition System\n# For ESP32-S3 with Gravity Analog EMG Sensor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import signal\nfrom scipy.stats import skew, kurtosis\nimport os\nimport glob\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n# Machine Learning imports\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport joblib\n\n# Add TensorFlow/Keras imports for CNN-LSTM\ntry:\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential, load_model\n    from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n    from tensorflow.keras.utils import to_categorical\n    TF_AVAILABLE = True\nexcept ImportError:\n    print(\"TensorFlow not available. CNN-LSTM model will be skipped.\")\n    TF_AVAILABLE = False\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Data Loading and Exploration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_emg_data(data_folder='data/'):\n    \"\"\"Load all EMG data files from the specified folder\"\"\"\n    all_data = []\n    \n    # Get all txt files\n    file_pattern = os.path.join(data_folder, '*.txt')\n    files = glob.glob(file_pattern)\n    \n    for file_path in files:\n        filename = os.path.basename(file_path)\n        print(f\"Loading {filename}...\")\n        \n        # Read the file\n        with open(file_path, 'r') as f:\n            lines = f.readlines()\n        \n        # Parse each line\n        for line in lines:\n            if line.strip():\n                parts = line.strip().split(',')\n                if len(parts) >= 3:\n                    all_data.append({\n                        'timestamp': float(parts[0]),\n                        'value': float(parts[1]),\n                        'label': int(parts[2]),\n                        'filename': filename\n                    })\n    \n    df = pd.DataFrame(all_data)\n    return df\n\n# Load data\ndf = load_emg_data('data/')\nprint(f\"\\nTotal samples loaded: {len(df)}\")\nprint(f\"Pinch samples (label=2): {len(df[df['label']==2])}\")\nprint(f\"Rotate samples (label=1): {len(df[df['label']==1])}\")\nprint(f\"\\nData statistics:\")\nprint(df.describe())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Signal Preprocessing and Filtering"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class EMGPreprocessor:\n    def __init__(self, sampling_freq=1000):\n        self.fs = sampling_freq\n        \n    def butter_bandpass(self, lowcut, highcut, order=4):\n        \"\"\"Create Butterworth bandpass filter\"\"\"\n        nyq = 0.5 * self.fs\n        low = lowcut / nyq\n        high = highcut / nyq\n        b, a = signal.butter(order, [low, high], btype='band')\n        return b, a\n    \n    def butter_lowpass(self, cutoff, order=4):\n        \"\"\"Create Butterworth lowpass filter\"\"\"\n        nyq = 0.5 * self.fs\n        normal_cutoff = cutoff / nyq\n        b, a = signal.butter(order, normal_cutoff, btype='low')\n        return b, a\n    \n    def apply_filter(self, data, filter_type='bandpass', lowcut=20, highcut=450):\n        \"\"\"Apply filter to EMG signal\"\"\"\n        if filter_type == 'bandpass':\n            b, a = self.butter_bandpass(lowcut, highcut)\n        else:  # lowpass\n            b, a = self.butter_lowpass(lowcut)\n        \n        filtered = signal.filtfilt(b, a, data)\n        return filtered\n    \n    def remove_dc_offset(self, data):\n        \"\"\"Remove DC offset from signal\"\"\"\n        return data - np.mean(data)\n    \n    def rectify(self, data):\n        \"\"\"Full-wave rectification\"\"\"\n        return np.abs(data)\n    \n    def moving_average(self, data, window_size=50):\n        \"\"\"Apply moving average smoothing\"\"\"\n        return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n\n# Initialize preprocessor\npreprocessor = EMGPreprocessor(sampling_freq=1000)\n\n# Apply preprocessing to the entire dataset\ndf['filtered_value'] = preprocessor.apply_filter(df['value'].values, 'lowpass', 20)\ndf['rectified'] = preprocessor.rectify(df['filtered_value'].values)\n\n# Visualize preprocessing effects\nfig, axes = plt.subplots(3, 1, figsize=(12, 8))\nsample_size = 2000\naxes[0].plot(df['value'][:sample_size], alpha=0.7)\naxes[0].set_title('Raw EMG Signal')\naxes[0].set_ylabel('Amplitude')\naxes[1].plot(df['filtered_value'][:sample_size], alpha=0.7, color='orange')\naxes[1].set_title('Filtered EMG Signal (20Hz Low-pass)')\naxes[1].set_ylabel('Amplitude')\naxes[2].plot(df['rectified'][:sample_size], alpha=0.7, color='green')\naxes[2].set_title('Rectified EMG Signal')\naxes[2].set_ylabel('Amplitude')\naxes[2].set_xlabel('Sample')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Feature Extraction"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class FeatureExtractor:\n    def __init__(self, window_size=50, step_size=10):\n        self.window_size = window_size\n        self.step_size = step_size\n    \n    def extract_time_domain_features(self, window):\n        \"\"\"Extract time-domain features from EMG window\"\"\"\n        features = {}\n        \n        # Statistical features\n        features['mean'] = np.mean(window)\n        features['std'] = np.std(window)\n        features['var'] = np.var(window)\n        features['rms'] = np.sqrt(np.mean(window**2))\n        features['mad'] = np.mean(np.abs(window - np.mean(window)))\n        \n        # Mean Absolute Value (MAV)\n        features['mav'] = np.mean(np.abs(window))\n        \n        # Zero Crossing Rate (ZCR)\n        zero_crossings = np.where(np.diff(np.sign(window)))[0]\n        features['zcr'] = len(zero_crossings)\n        \n        # Slope Sign Changes (SSC)\n        diff = np.diff(window)\n        ssc = np.sum(diff[:-1] * diff[1:] < 0)\n        features['ssc'] = ssc\n        \n        # Waveform Length (WL)\n        features['wl'] = np.sum(np.abs(np.diff(window)))\n        \n        # Willison Amplitude (WAMP)\n        threshold = 0.05 * np.max(np.abs(window))\n        features['wamp'] = np.sum(np.abs(np.diff(window)) > threshold)\n        \n        # Higher order statistics\n        features['skewness'] = skew(window)\n        features['kurtosis'] = kurtosis(window)\n        \n        # Percentiles\n        features['p25'] = np.percentile(window, 25)\n        features['p50'] = np.percentile(window, 50)\n        features['p75'] = np.percentile(window, 75)\n        features['iqr'] = features['p75'] - features['p25']\n        \n        # Energy\n        features['energy'] = np.sum(window**2)\n        \n        # Peak-to-peak amplitude\n        features['peak_to_peak'] = np.max(window) - np.min(window)\n        \n        return features\n    \n    def extract_frequency_domain_features(self, window, fs=1000):\n        \"\"\"Extract frequency-domain features using FFT\"\"\"\n        features = {}\n        \n        # Compute FFT\n        fft_vals = np.fft.rfft(window)\n        fft_freqs = np.fft.rfftfreq(len(window), 1/fs)\n        magnitude = np.abs(fft_vals)\n        \n        # Mean frequency\n        features['mean_freq'] = np.sum(fft_freqs * magnitude) / np.sum(magnitude)\n        \n        # Median frequency\n        cumsum = np.cumsum(magnitude)\n        features['median_freq'] = fft_freqs[np.where(cumsum >= cumsum[-1]/2)[0][0]]\n        \n        # Peak frequency\n        features['peak_freq'] = fft_freqs[np.argmax(magnitude)]\n        \n        # Band power (0-50Hz, 50-150Hz, 150-250Hz, 250-500Hz)\n        bands = [(0, 50), (50, 150), (150, 250), (250, 500)]\n        for i, (low, high) in enumerate(bands):\n            band_idx = np.where((fft_freqs >= low) & (fft_freqs < high))\n            features[f'band_power_{i}'] = np.sum(magnitude[band_idx]**2)\n        \n        return features\n    \n    def create_feature_dataset(self, data, labels):\n        \"\"\"Create windowed feature dataset\"\"\"\n        feature_list = []\n        label_list = []\n        \n        for i in range(0, len(data) - self.window_size, self.step_size):\n            window = data[i:i + self.window_size]\n            window_labels = labels[i:i + self.window_size]\n            \n            # Use majority voting for label\n            label = np.bincount(window_labels).argmax()\n            \n            # Extract features\n            time_features = self.extract_time_domain_features(window)\n            freq_features = self.extract_frequency_domain_features(window)\n            \n            # Combine features\n            all_features = {**time_features, **freq_features}\n            \n            feature_list.append(all_features)\n            label_list.append(label)\n        \n        return pd.DataFrame(feature_list), np.array(label_list)\n\n# Extract features\nextractor = FeatureExtractor(window_size=50, step_size=10)\nX, y = extractor.create_feature_dataset(\n    df['filtered_value'].values,\n    df['label'].values\n)\nprint(f\"Feature matrix shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"\\nFeature names: {list(X.columns)}\")\n\n# Convert labels: 1 (rotate) -> 0, 2 (pinch) -> 1\ny_binary = (y == 2).astype(int)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Feature Selection and Visualization\nVisualize feature importance using Random Forest"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_temp.fit(X, y_binary)\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_temp.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\nplt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])\nplt.xlabel('Feature Importance')\nplt.title('Top 15 Most Important Features')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(\"Top 10 features:\")\nprint(feature_importance.head(10))\n\n# Select top features\ntop_features = feature_importance['feature'][:15].values\nX_selected = X[top_features]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Model Training and Evaluation\nSplit data"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X_selected, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n)\n\n# Normalize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Dictionary to store models and results\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n    'Neural Network': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),\n    'KNN': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Add CNN-LSTM if TensorFlow is available\nif TF_AVAILABLE:\n    # Prepare data for CNN-LSTM\n    def create_cnn_lstm_data(data, labels, window_size=50, step_size=10):\n        \"\"\"Create windowed dataset for CNN-LSTM\"\"\"\n        X = []\n        y = []\n        \n        for i in range(0, len(data) - window_size, step_size):\n            window = data[i:i + window_size]\n            window_labels = labels[i:i + window_size]\n            \n            # Use majority voting for label\n            label = np.bincount(window_labels).argmax()\n            \n            X.append(window)\n            y.append(label)\n        \n        return np.array(X), np.array(y)\n    \n    # Create CNN-LSTM dataset\n    X_cnn, y_cnn = create_cnn_lstm_data(\n        df['filtered_value'].values,\n        df['label'].values\n    )\n    \n    # Convert labels to binary: 1 (rotate) -> 0, 2 (pinch) -> 1\n    y_cnn_binary = (y_cnn == 2).astype(int)\n    \n    # Split data\n    X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n        X_cnn, y_cnn_binary, test_size=0.2, random_state=42, stratify=y_cnn_binary\n    )\n    \n    # Reshape for CNN-LSTM [samples, timesteps, features]\n    X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))\n    X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n    \n    # Define CNN-LSTM model\n    def create_cnn_lstm_model(input_shape):\n        model = Sequential()\n        \n        # CNN layers\n        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n        model.add(BatchNormalization())\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Dropout(0.3))\n        \n        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Dropout(0.3))\n        \n        # LSTM layers\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Dropout(0.3))\n        model.add(LSTM(32))\n        model.add(Dropout(0.3))\n        \n        # Output layer\n        model.add(Dense(1, activation='sigmoid'))\n        \n        # Compile model\n        model.compile(\n            optimizer=Adam(learning_rate=0.001),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    # Create model\n    cnn_lstm_model = create_cnn_lstm_model((X_train_cnn.shape[1], 1))\n    cnn_lstm_model.summary()\n    \n    # Add to models dictionary\n    models['CNN-LSTM'] = cnn_lstm_model\n\nresults = {}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Train CNN-LSTM separately\n    if name == 'CNN-LSTM':\n        # Define callbacks\n        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n        model_checkpoint = ModelCheckpoint(\n            'cnn_lstm_best_model.h5', \n            monitor='val_accuracy', \n            save_best_only=True, \n            mode='max'\n        )\n        \n        # Train model\n        history = model.fit(\n            X_train_cnn, y_train_cnn,\n            epochs=50,\n            batch_size=32,\n            validation_split=0.2,\n            callbacks=[early_stopping, model_checkpoint],\n            verbose=1\n        )\n        \n        # Load best model\n        model = load_model('cnn_lstm_best_model.h5')\n        \n        # Evaluate on test set\n        test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n        y_pred = (model.predict(X_test_cnn) > 0.5).astype(int).flatten()\n        y_prob = model.predict(X_test_cnn).flatten()\n        \n        # Get validation accuracy from training history\n        val_accuracy = max(history.history['val_accuracy'])\n        \n        # Store results\n        results[name] = {\n            'model': model,\n            'accuracy': test_accuracy,\n            'val_accuracy': val_accuracy,\n            'cost_function': 'binary_crossentropy',\n            'predictions': y_pred,\n            'probabilities': y_prob,\n            'history': history\n        }\n        \n        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n        print(f\"Cost Function: binary_crossentropy\")\n        \n        # Plot training history\n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.plot(history.history['loss'], label='Train Loss')\n        plt.plot(history.history['val_loss'], label='Validation Loss')\n        plt.title('Model Loss')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history.history['accuracy'], label='Train Accuracy')\n        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Model Accuracy')\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        \n    else:  # Traditional ML models\n        # Use scaled data for SVM, NN, and KNN\n        if name in ['SVM', 'Neural Network', 'KNN']:\n            model.fit(X_train_scaled, y_train)\n            y_pred = model.predict(X_test_scaled)\n            y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n            \n            # Cross-validation\n            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n        else:\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n            \n            # Cross-validation\n            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Determine cost function\n        if name == 'Random Forest':\n            cost_function = 'Gini impurity'\n        elif name == 'Gradient Boosting':\n            cost_function = 'Deviance'\n        elif name == 'SVM':\n            cost_function = 'Hinge loss'\n        elif name == 'Neural Network':\n            cost_function = 'Log loss'\n        elif name == 'KNN':\n            cost_function = 'N/A (instance-based)'\n        \n        # Store results\n        results[name] = {\n            'model': model,\n            'accuracy': accuracy,\n            'val_accuracy': cv_scores.mean(),\n            'cost_function': cost_function,\n            'cv_scores': cv_scores,\n            'predictions': y_pred,\n            'probabilities': y_prob\n        }\n        \n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Validation Accuracy (CV): {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n        print(f\"Cost Function: {cost_function}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test if name != 'CNN-LSTM' else y_test_cnn, \n                              y_pred, \n                              target_names=['Rotate', 'Pinch']))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Model Comparison and Selection\nCompare models"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model_names = list(results.keys())\ntest_accuracies = [results[name]['accuracy'] for name in model_names]\nval_accuracies = [results[name]['val_accuracy'] for name in model_names]\n\n# For traditional models, get CV std\ncv_stds = []\nfor name in model_names:\n    if name == 'CNN-LSTM':\n        cv_stds.append(0)  # No std for CNN-LSTM\n    else:\n        cv_stds.append(results[name]['cv_scores'].std())\n\ncomparison_df = pd.DataFrame({\n    'Model': model_names,\n    'Test Accuracy': test_accuracies,\n    'Validation Accuracy': val_accuracies,\n    'CV Std': cv_stds,\n    'Cost Function': [results[name]['cost_function'] for name in model_names]\n}).sort_values('Test Accuracy', ascending=False)\n\nprint(\"\\nModel Comparison:\")\nprint(comparison_df)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Test Accuracy comparison\naxes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], color='skyblue')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Test Accuracy Comparison')\naxes[0].set_ylim([0.7, 1.0])\naxes[0].tick_params(axis='x', rotation=45)\nfor i, v in enumerate(comparison_df['Test Accuracy']):\n    axes[0].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n\n# Validation Accuracy comparison with error bars\naxes[1].bar(comparison_df['Model'], comparison_df['Validation Accuracy'], color='lightgreen')\naxes[1].errorbar(range(len(comparison_df)), comparison_df['Validation Accuracy'], \n                 yerr=comparison_df['CV Std'], fmt='none', color='black', capsize=5)\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Validation Accuracy Comparison')\naxes[1].set_ylim([0.7, 1.0])\naxes[1].set_xticks(range(len(comparison_df)))\naxes[1].set_xticklabels(comparison_df['Model'], rotation=45)\nfor i, v in enumerate(comparison_df['Validation Accuracy']):\n    axes[1].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# Select best model based on test accuracy\nbest_model_name = comparison_df.iloc[0]['Model']\nbest_model = results[best_model_name]['model']\nprint(f\"\\nBest model: {best_model_name} with Test Accuracy: {comparison_df.iloc[0]['Test Accuracy']:.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Confusion Matrix and Detailed Evaluation\nPlot confusion matrices for all models"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "num_models = len(results)\nrows = (num_models + 2) // 3  # Calculate rows needed (3 models per row)\nfig, axes = plt.subplots(rows, 3, figsize=(15, 5*rows))\naxes = axes.ravel()  # Flatten axes array\n\nfor idx, (name, result) in enumerate(results.items()):\n    cm = confusion_matrix(y_test if name != 'CNN-LSTM' else y_test_cnn, result['predictions'])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n                xticklabels=['Rotate', 'Pinch'], yticklabels=['Rotate', 'Pinch'])\n    axes[idx].set_title(f'{name}\\nAccuracy: {result[\"accuracy\"]:.3f}')\n    axes[idx].set_ylabel('True Label')\n    axes[idx].set_xlabel('Predicted Label')\n\n# Hide any unused subplots\nfor idx in range(num_models, len(axes)):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Save Best Model for Deployment\nSave the best model and preprocessing parameters"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if best_model_name == 'CNN-LSTM':\n    # Save Keras model\n    best_model.save('best_cnn_lstm_model.h5')\n    model_data = {\n        'model_type': best_model_name,\n        'model_path': 'best_cnn_lstm_model.h5',\n        'window_size': extractor.window_size,\n        'step_size': extractor.step_size\n    }\nelse:\n    # Save scikit-learn model\n    model_data = {\n        'model': best_model,\n        'scaler': scaler if best_model_name in ['SVM', 'Neural Network', 'KNN'] else None,\n        'feature_names': top_features,\n        'window_size': extractor.window_size,\n        'step_size': extractor.step_size,\n        'model_type': best_model_name\n    }\n    joblib.dump(model_data, 'emg_gesture_model.pkl')\n\nprint(f\"Model saved for {best_model_name}\")\n\n# Also save as separate components for flexibility\nif best_model_name != 'CNN-LSTM':\n    joblib.dump(best_model, 'best_model.pkl')\n    joblib.dump(scaler, 'scaler.pkl')\n    np.save('feature_names.npy', top_features)\n    print(\"Individual components saved:\")\n    print(\"- best_model.pkl\")\n    print(\"- scaler.pkl\")\n    print(\"- feature_names.npy\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Real-time Prediction Function"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def predict_gesture_realtime(emg_buffer, model_data):\n    \"\"\"\n    Make real-time prediction from EMG buffer\n    \n    Args:\n        emg_buffer: numpy array of EMG values (length = window_size)\n        model_data: dictionary containing model and preprocessing parameters\n    \n    Returns:\n        gesture: 'pinch' or 'rotate'\n        confidence: probability of the predicted gesture\n    \"\"\"\n    if model_data['model_type'] == 'CNN-LSTM':\n        # Load Keras model\n        model = load_model(model_data['model_path'])\n        \n        # Reshape for CNN-LSTM\n        emg_buffer = emg_buffer.reshape((1, emg_buffer.shape[0], 1))\n        \n        # Make prediction\n        prediction_prob = model.predict(emg_buffer)[0][0]\n        prediction = 1 if prediction_prob > 0.5 else 0\n        confidence = prediction_prob if prediction == 1 else 1 - prediction_prob\n        \n    else:\n        # Extract features\n        extractor = FeatureExtractor(window_size=len(emg_buffer), step_size=1)\n        features = extractor.extract_time_domain_features(emg_buffer)\n        freq_features = extractor.extract_frequency_domain_features(emg_buffer)\n        features.update(freq_features)\n        \n        # Create feature dataframe\n        feature_df = pd.DataFrame([features])\n        \n        # Select relevant features\n        feature_df = feature_df[model_data['feature_names']]\n        \n        # Scale if necessary\n        if model_data['scaler'] is not None:\n            feature_values = model_data['scaler'].transform(feature_df)\n        else:\n            feature_values = feature_df.values\n        \n        # Make prediction\n        model = model_data['model']\n        prediction = model.predict(feature_values)[0]\n        probability = model.predict_proba(feature_values)[0]\n        \n        confidence = probability[prediction]\n    \n    gesture = 'pinch' if prediction == 1 else 'rotate'\n    return gesture, confidence\n\n# Test the real-time prediction function\ntest_buffer = df['filtered_value'].values[:50]  # Get a test window\nif best_model_name == 'CNN-LSTM':\n    loaded_model_data = {\n        'model_type': 'CNN-LSTM',\n        'model_path': 'best_cnn_lstm_model.h5',\n        'window_size': extractor.window_size,\n        'step_size': extractor.step_size\n    }\nelse:\n    loaded_model_data = joblib.load('emg_gesture_model.pkl')\n\ngesture, confidence = predict_gesture_realtime(test_buffer, loaded_model_data)\nprint(f\"\\nTest prediction: {gesture} (confidence: {confidence:.2%})\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. ESP32 Integration Code"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "esp32_code = \"\"\"\n// ESP32-S3 EMG Gesture Recognition\n// Using Gravity Analog EMG Sensor by OYMotion\n#include <Arduino.h>\nconst int EMG_PIN = 34;  // Analog input pin\nconst int WINDOW_SIZE = 50;\nconst int STEP_SIZE = 10;\nconst int SAMPLING_RATE = 1000;  // Hz\nfloat emg_buffer[WINDOW_SIZE];\nint buffer_index = 0;\n// Feature extraction functions\nfloat calculate_mean(float* data, int len) {\n    float sum = 0;\n    for(int i = 0; i < len; i++) {\n        sum += data[i];\n    }\n    return sum / len;\n}\nfloat calculate_std(float* data, int len) {\n    float mean = calculate_mean(data, len);\n    float sum = 0;\n    for(int i = 0; i < len; i++) {\n        sum += pow(data[i] - mean, 2);\n    }\n    return sqrt(sum / len);\n}\nfloat calculate_rms(float* data, int len) {\n    float sum = 0;\n    for(int i = 0; i < len; i++) {\n        sum += pow(data[i], 2);\n    }\n    return sqrt(sum / len);\n}\nint calculate_zcr(float* data, int len) {\n    int count = 0;\n    for(int i = 1; i < len; i++) {\n        if((data[i] >= 0 && data[i-1] < 0) || \n           (data[i] < 0 && data[i-1] >= 0)) {\n            count++;\n        }\n    }\n    return count;\n}\nvoid setup() {\n    Serial.begin(115200);\n    pinMode(EMG_PIN, INPUT);\n}\nvoid loop() {\n    // Read EMG value\n    int raw_value = analogRead(EMG_PIN);\n    float emg_value = (raw_value / 4095.0) * 4000;  // Convert to 0-4000 range\n    \n    // Add to buffer\n    emg_buffer[buffer_index] = emg_value;\n    buffer_index = (buffer_index + 1) % WINDOW_SIZE;\n    \n    // Process when buffer is full\n    if(buffer_index == 0) {\n        // Extract features\n        float mean = calculate_mean(emg_buffer, WINDOW_SIZE);\n        float std = calculate_std(emg_buffer, WINDOW_SIZE);\n        float rms = calculate_rms(emg_buffer, WINDOW_SIZE);\n        int zcr = calculate_zcr(emg_buffer, WINDOW_SIZE);\n        \n        // Send features to serial for processing\n        Serial.print(\"FEATURES:\");\n        Serial.print(mean); Serial.print(\",\");\n        Serial.print(std); Serial.print(\",\");\n        Serial.print(rms); Serial.print(\",\");\n        Serial.print(zcr);\n        Serial.println();\n    }\n    \n    delay(1000 / SAMPLING_RATE);  // Maintain sampling rate\n}\n\"\"\"\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ESP32 Integration Code saved in the script\")\nprint(\"=\"*50)\n\n# Save ESP32 code to file\nwith open('esp32_emg_gesture.ino', 'w') as f:\n    f.write(esp32_code)\nprint(\"ESP32 code saved as 'esp32_emg_gesture.ino'\")\nprint(\"\\nComplete EMG Gesture Recognition System Ready!\")\nprint(\"\\nNext steps:\")\nprint(\"1. Upload the ESP32 code to your microcontroller\")\nprint(\"2. Use the saved model for real-time predictions\")\nprint(\"3. Fine-tune the model with more data if needed\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}