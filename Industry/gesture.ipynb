{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EMG Gesture Recognition System\n",
        "# For ESP32-S3 with Gravity Analog EMG Sensor\n",
        "# Modified for 3-class classification (rest, rotate, pinch)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "from scipy.stats import skew, kurtosis\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Machine Learning imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import joblib\n",
        "# Add TensorFlow/Keras imports for CNN-LSTM\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, load_model\n",
        "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    TF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"TensorFlow not available. CNN-LSTM model will be skipped.\")\n",
        "    TF_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30aee706",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9917f206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_emg_data(data_folder='threshold_data/'):\n",
        "    \"\"\"Load all EMG data files from the specified folder\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Get all txt files\n",
        "    file_pattern = os.path.join(data_folder, '*.txt')\n",
        "    files = glob.glob(file_pattern)\n",
        "    \n",
        "    for file_path in files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"Loading {filename}...\")\n",
        "        \n",
        "        # Read the file\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Parse each line\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) >= 3:\n",
        "                    all_data.append({\n",
        "                        'timestamp': float(parts[0]),\n",
        "                        'value': float(parts[1]),\n",
        "                        'label': int(parts[2]),\n",
        "                        'filename': filename\n",
        "                    })\n",
        "    \n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "df = load_emg_data('threshold_data/')\n",
        "print(f\"\\nTotal samples loaded: {len(df)}\")\n",
        "print(f\"Rest samples (label=0): {len(df[df['label']==0])}\")\n",
        "print(f\"Rotate samples (label=1): {len(df[df['label']==1])}\")\n",
        "print(f\"Pinch samples (label=2): {len(df[df['label']==2])}\")\n",
        "print(f\"\\nData statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0acf8677",
      "metadata": {},
      "source": [
        "## 2. Signal Preprocessing and Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63749ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EMGPreprocessor:\n",
        "    def __init__(self, sampling_freq=1000):\n",
        "        self.fs = sampling_freq\n",
        "        \n",
        "    def butter_bandpass(self, lowcut, highcut, order=4):\n",
        "        \"\"\"Create Butterworth bandpass filter\"\"\"\n",
        "        nyq = 0.5 * self.fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = signal.butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "    \n",
        "    def butter_lowpass(self, cutoff, order=4):\n",
        "        \"\"\"Create Butterworth lowpass filter\"\"\"\n",
        "        nyq = 0.5 * self.fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = signal.butter(order, normal_cutoff, btype='low')\n",
        "        return b, a\n",
        "    \n",
        "    def apply_filter(self, data, filter_type='bandpass', lowcut=20, highcut=450):\n",
        "        \"\"\"Apply filter to EMG signal\"\"\"\n",
        "        if filter_type == 'bandpass':\n",
        "            b, a = self.butter_bandpass(lowcut, highcut)\n",
        "        else:  # lowpass\n",
        "            b, a = self.butter_lowpass(lowcut)\n",
        "        \n",
        "        filtered = signal.filtfilt(b, a, data)\n",
        "        return filtered\n",
        "    \n",
        "    def remove_dc_offset(self, data):\n",
        "        \"\"\"Remove DC offset from signal\"\"\"\n",
        "        return data - np.mean(data)\n",
        "    \n",
        "    def rectify(self, data):\n",
        "        \"\"\"Full-wave rectification\"\"\"\n",
        "        return np.abs(data)\n",
        "    \n",
        "    def moving_average(self, data, window_size=50):\n",
        "        \"\"\"Apply moving average smoothing\"\"\"\n",
        "        return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = EMGPreprocessor(sampling_freq=1000)\n",
        "\n",
        "# Apply preprocessing to the entire dataset\n",
        "df['filtered_value'] = preprocessor.apply_filter(df['value'].values, 'lowpass', 20)\n",
        "df['rectified'] = preprocessor.rectify(df['filtered_value'].values)\n",
        "\n",
        "# Visualize preprocessing effects\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
        "sample_size = 2000\n",
        "axes[0].plot(df['value'][:sample_size], alpha=0.7)\n",
        "axes[0].set_title('Raw EMG Signal')\n",
        "axes[0].set_ylabel('Amplitude')\n",
        "axes[1].plot(df['filtered_value'][:sample_size], alpha=0.7, color='orange')\n",
        "axes[1].set_title('Filtered EMG Signal (20Hz Low-pass)')\n",
        "axes[1].set_ylabel('Amplitude')\n",
        "axes[2].plot(df['rectified'][:sample_size], alpha=0.7, color='green')\n",
        "axes[2].set_title('Rectified EMG Signal')\n",
        "axes[2].set_ylabel('Amplitude')\n",
        "axes[2].set_xlabel('Sample')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d855b0",
      "metadata": {},
      "source": [
        "## 3. Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4552200",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, window_size=50, step_size=10):\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "    \n",
        "    def extract_time_domain_features(self, window):\n",
        "        \"\"\"Extract time-domain features from EMG window\"\"\"\n",
        "        features = {}\n",
        "        \n",
        "        # Statistical features\n",
        "        features['mean'] = np.mean(window)\n",
        "        features['std'] = np.std(window)\n",
        "        features['var'] = np.var(window)\n",
        "        features['rms'] = np.sqrt(np.mean(window**2))\n",
        "        features['mad'] = np.mean(np.abs(window - np.mean(window)))\n",
        "        \n",
        "        # Mean Absolute Value (MAV)\n",
        "        features['mav'] = np.mean(np.abs(window))\n",
        "        \n",
        "        # Zero Crossing Rate (ZCR)\n",
        "        zero_crossings = np.where(np.diff(np.sign(window)))[0]\n",
        "        features['zcr'] = len(zero_crossings)\n",
        "        \n",
        "        # Slope Sign Changes (SSC)\n",
        "        diff = np.diff(window)\n",
        "        ssc = np.sum(diff[:-1] * diff[1:] < 0)\n",
        "        features['ssc'] = ssc\n",
        "        \n",
        "        # Waveform Length (WL)\n",
        "        features['wl'] = np.sum(np.abs(np.diff(window)))\n",
        "        \n",
        "        # Willison Amplitude (WAMP)\n",
        "        threshold = 0.05 * np.max(np.abs(window))\n",
        "        features['wamp'] = np.sum(np.abs(np.diff(window)) > threshold)\n",
        "        \n",
        "        # Higher order statistics\n",
        "        features['skewness'] = skew(window)\n",
        "        features['kurtosis'] = kurtosis(window)\n",
        "        \n",
        "        # Percentiles\n",
        "        features['p25'] = np.percentile(window, 25)\n",
        "        features['p50'] = np.percentile(window, 50)\n",
        "        features['p75'] = np.percentile(window, 75)\n",
        "        features['iqr'] = features['p75'] - features['p25']\n",
        "        \n",
        "        # Energy\n",
        "        features['energy'] = np.sum(window**2)\n",
        "        \n",
        "        # Peak-to-peak amplitude\n",
        "        features['peak_to_peak'] = np.max(window) - np.min(window)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def extract_frequency_domain_features(self, window, fs=1000):\n",
        "        \"\"\"Extract frequency-domain features using FFT\"\"\"\n",
        "        features = {}\n",
        "        \n",
        "        # Compute FFT\n",
        "        fft_vals = np.fft.rfft(window)\n",
        "        fft_freqs = np.fft.rfftfreq(len(window), 1/fs)\n",
        "        magnitude = np.abs(fft_vals)\n",
        "        \n",
        "        # Mean frequency\n",
        "        features['mean_freq'] = np.sum(fft_freqs * magnitude) / np.sum(magnitude)\n",
        "        \n",
        "        # Median frequency\n",
        "        cumsum = np.cumsum(magnitude)\n",
        "        features['median_freq'] = fft_freqs[np.where(cumsum >= cumsum[-1]/2)[0][0]]\n",
        "        \n",
        "        # Peak frequency\n",
        "        features['peak_freq'] = fft_freqs[np.argmax(magnitude)]\n",
        "        \n",
        "        # Band power (0-50Hz, 50-150Hz, 150-250Hz, 250-500Hz)\n",
        "        bands = [(0, 50), (50, 150), (150, 250), (250, 500)]\n",
        "        for i, (low, high) in enumerate(bands):\n",
        "            band_idx = np.where((fft_freqs >= low) & (fft_freqs < high))\n",
        "            features[f'band_power_{i}'] = np.sum(magnitude[band_idx]**2)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def create_feature_dataset(self, data, labels):\n",
        "        \"\"\"Create windowed feature dataset\"\"\"\n",
        "        feature_list = []\n",
        "        label_list = []\n",
        "        \n",
        "        for i in range(0, len(data) - self.window_size, self.step_size):\n",
        "            window = data[i:i + self.window_size]\n",
        "            window_labels = labels[i:i + self.window_size]\n",
        "            \n",
        "            # Use majority voting for label\n",
        "            label = np.bincount(window_labels).argmax()\n",
        "            \n",
        "            # Extract features\n",
        "            time_features = self.extract_time_domain_features(window)\n",
        "            freq_features = self.extract_frequency_domain_features(window)\n",
        "            \n",
        "            # Combine features\n",
        "            all_features = {**time_features, **freq_features}\n",
        "            \n",
        "            feature_list.append(all_features)\n",
        "            label_list.append(label)\n",
        "        \n",
        "        return pd.DataFrame(feature_list), np.array(label_list)\n",
        "\n",
        "# Extract features\n",
        "extractor = FeatureExtractor(window_size=50, step_size=10)\n",
        "X, y = extractor.create_feature_dataset(\n",
        "    df['filtered_value'].values,\n",
        "    df['label'].values\n",
        ")\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n",
        "\n",
        "# No need to convert labels to binary - we have 3 classes: 0, 1, 2\n",
        "# y_binary = (y == 2).astype(int)  # Removed this line\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603385ef",
      "metadata": {},
      "source": [
        "## 4. Feature Selection and Visualization\n",
        "Visualize feature importance using Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6239df3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_temp.fit(X, y)  # Use multi-class labels\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_temp.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 15 Most Important Features')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Select top features\n",
        "top_features = feature_importance['feature'][:15].values\n",
        "X_selected = X[top_features]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fe3c8b",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation\n",
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a505047",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Dictionary to store models and results\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Neural Network': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Add CNN-LSTM if TensorFlow is available\n",
        "if TF_AVAILABLE:\n",
        "    # Prepare data for CNN-LSTM\n",
        "    def create_cnn_lstm_data(data, labels, window_size=50, step_size=10):\n",
        "        \"\"\"Create windowed dataset for CNN-LSTM\"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        for i in range(0, len(data) - window_size, step_size):\n",
        "            window = data[i:i + window_size]\n",
        "            window_labels = labels[i:i + window_size]\n",
        "            \n",
        "            # Use majority voting for label\n",
        "            label = np.bincount(window_labels).argmax()\n",
        "            \n",
        "            X.append(window)\n",
        "            y.append(label)\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    # Create CNN-LSTM dataset\n",
        "    X_cnn, y_cnn = create_cnn_lstm_data(\n",
        "        df['filtered_value'].values,\n",
        "        df['label'].values\n",
        "    )\n",
        "    \n",
        "    # Convert labels to categorical (one-hot encoding)\n",
        "    y_cnn_categorical = to_categorical(y_cnn, num_classes=3)\n",
        "    \n",
        "    # Split data\n",
        "    X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
        "        X_cnn, y_cnn_categorical, test_size=0.2, random_state=42, stratify=y_cnn\n",
        "    )\n",
        "    \n",
        "    # Reshape for CNN-LSTM [samples, timesteps, features]\n",
        "    X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))\n",
        "    X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n",
        "    \n",
        "    # Define CNN-LSTM model for 3 classes\n",
        "    def create_cnn_lstm_model(input_shape):\n",
        "        model = Sequential()\n",
        "        \n",
        "        # CNN layers\n",
        "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "        \n",
        "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(0.3))\n",
        "        \n",
        "        # LSTM layers\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.3))\n",
        "        \n",
        "        # Output layer - 3 classes with softmax\n",
        "        model.add(Dense(3, activation='softmax'))\n",
        "        \n",
        "        # Compile model with categorical crossentropy\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    # Create model\n",
        "    cnn_lstm_model = create_cnn_lstm_model((X_train_cnn.shape[1], 1))\n",
        "    cnn_lstm_model.summary()\n",
        "    \n",
        "    # Add to models dictionary\n",
        "    models['CNN-LSTM'] = cnn_lstm_model\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train CNN-LSTM separately\n",
        "    if name == 'CNN-LSTM':\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        model_checkpoint = ModelCheckpoint(\n",
        "            'cnn_lstm_best_model.h5', \n",
        "            monitor='val_accuracy', \n",
        "            save_best_only=True, \n",
        "            mode='max'\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train_cnn, y_train_cnn,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, model_checkpoint],\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        # Load best model\n",
        "        model = load_model('cnn_lstm_best_model.h5')\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
        "        y_pred = model.predict(X_test_cnn)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true_classes = np.argmax(y_test_cnn, axis=1)\n",
        "        y_prob = y_pred\n",
        "        \n",
        "        # Get validation accuracy from training history\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        \n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': test_accuracy,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'cost_function': 'categorical_crossentropy',\n",
        "            'predictions': y_pred_classes,\n",
        "            'probabilities': y_prob,\n",
        "            'history': history,\n",
        "            'y_true': y_true_classes\n",
        "        }\n",
        "        \n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        print(f\"Cost Function: categorical_crossentropy\")\n",
        "        \n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    else:  # Traditional ML models\n",
        "        # Use scaled data for SVM, NN, and KNN\n",
        "        if name in ['SVM', 'Neural Network', 'KNN']:\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            y_prob = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
        "            \n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_prob = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "            \n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        # Determine cost function\n",
        "        if name == 'Random Forest':\n",
        "            cost_function = 'Gini impurity'\n",
        "        elif name == 'Gradient Boosting':\n",
        "            cost_function = 'Deviance'\n",
        "        elif name == 'SVM':\n",
        "            cost_function = 'Hinge loss'\n",
        "        elif name == 'Neural Network':\n",
        "            cost_function = 'Log loss'\n",
        "        elif name == 'KNN':\n",
        "            cost_function = 'N/A (instance-based)'\n",
        "        \n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'val_accuracy': cv_scores.mean(),\n",
        "            'cost_function': cost_function,\n",
        "            'cv_scores': cv_scores,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_prob,\n",
        "            'y_true': y_test\n",
        "        }\n",
        "        \n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Validation Accuracy (CV): {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "        print(f\"Cost Function: {cost_function}\")\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        results[name]['y_true'], \n",
        "        results[name]['predictions'], \n",
        "        target_names=['Rest', 'Rotate', 'Pinch']\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7393862",
      "metadata": {},
      "source": [
        "## 6. Model Comparison and Selection\n",
        "Compare models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb03bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_names = list(results.keys())\n",
        "test_accuracies = [results[name]['accuracy'] for name in model_names]\n",
        "val_accuracies = [results[name]['val_accuracy'] for name in model_names]\n",
        "\n",
        "# For traditional models, get CV std\n",
        "cv_stds = []\n",
        "for name in model_names:\n",
        "    if name == 'CNN-LSTM':\n",
        "        cv_stds.append(0)  # No std for CNN-LSTM\n",
        "    else:\n",
        "        cv_stds.append(results[name]['cv_scores'].std())\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Test Accuracy': test_accuracies,\n",
        "    'Validation Accuracy': val_accuracies,\n",
        "    'CV Std': cv_stds,\n",
        "    'Cost Function': [results[name]['cost_function'] for name in model_names]\n",
        "}).sort_values('Test Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "# Test Accuracy comparison\n",
        "axes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], color='skyblue')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Test Accuracy Comparison')\n",
        "axes[0].set_ylim([0.5, 1.0])\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
        "    axes[0].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "# Validation Accuracy comparison with error bars\n",
        "axes[1].bar(comparison_df['Model'], comparison_df['Validation Accuracy'], color='lightgreen')\n",
        "axes[1].errorbar(range(len(comparison_df)), comparison_df['Validation Accuracy'], \n",
        "                 yerr=comparison_df['CV Std'], fmt='none', color='black', capsize=5)\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy Comparison')\n",
        "axes[1].set_ylim([0.5, 1.0])\n",
        "axes[1].set_xticks(range(len(comparison_df)))\n",
        "axes[1].set_xticklabels(comparison_df['Model'], rotation=45)\n",
        "for i, v in enumerate(comparison_df['Validation Accuracy']):\n",
        "    axes[1].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model based on test accuracy\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "print(f\"\\nBest model: {best_model_name} with Test Accuracy: {comparison_df.iloc[0]['Test Accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f4f887",
      "metadata": {},
      "source": [
        "## 7. Confusion Matrix and Detailed Evaluation\n",
        "Plot confusion matrices for all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6df23b",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_models = len(results)\n",
        "rows = (num_models + 2) // 3  # Calculate rows needed (3 models per row)\n",
        "fig, axes = plt.subplots(rows, 3, figsize=(15, 5*rows))\n",
        "axes = axes.ravel()  # Flatten axes array\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(result['y_true'], result['predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['Rest', 'Rotate', 'Pinch'], yticklabels=['Rest', 'Rotate', 'Pinch'])\n",
        "    axes[idx].set_title(f'{name}\\nAccuracy: {result[\"accuracy\"]:.3f}')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "# Hide any unused subplots\n",
        "for idx in range(num_models, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f9cb90",
      "metadata": {},
      "source": [
        "## 8. Save Best Model for Deployment\n",
        "Save the best model and preprocessing parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd5bf21",
      "metadata": {},
      "outputs": [],
      "source": [
        "if best_model_name == 'CNN-LSTM':\n",
        "    # Save Keras model\n",
        "    best_model.save('best_cnn_lstm_model.h5')\n",
        "    model_data = {\n",
        "        'model_type': best_model_name,\n",
        "        'model_path': 'best_cnn_lstm_model.h5',\n",
        "        'window_size': extractor.window_size,\n",
        "        'step_size': extractor.step_size\n",
        "    }\n",
        "else:\n",
        "    # Save scikit-learn model\n",
        "    model_data = {\n",
        "        'model': best_model,\n",
        "        'scaler': scaler if best_model_name in ['SVM', 'Neural Network', 'KNN'] else None,\n",
        "        'feature_names': top_features,\n",
        "        'window_size': extractor.window_size,\n",
        "        'step_size': extractor.step_size,\n",
        "        'model_type': best_model_name\n",
        "    }\n",
        "    joblib.dump(model_data, 'emg_gesture_model.pkl')\n",
        "\n",
        "print(f\"Model saved for {best_model_name}\")\n",
        "\n",
        "# Also save as separate components for flexibility\n",
        "if best_model_name != 'CNN-LSTM':\n",
        "    joblib.dump(best_model, 'best_model.pkl')\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "    np.save('feature_names.npy', top_features)\n",
        "    print(\"Individual components saved:\")\n",
        "    print(\"- best_model.pkl\")\n",
        "    print(\"- scaler.pkl\")\n",
        "    print(\"- feature_names.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d863a47",
      "metadata": {},
      "source": [
        "## 9. Real-time Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c6cadc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_gesture_realtime(emg_buffer, model_data):\n",
        "    \"\"\"\n",
        "    Make real-time prediction from EMG buffer\n",
        "    \n",
        "    Args:\n",
        "        emg_buffer: numpy array of EMG values (length = window_size)\n",
        "        model_data: dictionary containing model and preprocessing parameters\n",
        "    \n",
        "    Returns:\n",
        "        gesture: 'rest', 'rotate', or 'pinch'\n",
        "        confidence: probability of the predicted gesture\n",
        "    \"\"\"\n",
        "    if model_data['model_type'] == 'CNN-LSTM':\n",
        "        # Load Keras model\n",
        "        model = load_model(model_data['model_path'])\n",
        "        \n",
        "        # Reshape for CNN-LSTM\n",
        "        emg_buffer = emg_buffer.reshape((1, emg_buffer.shape[0], 1))\n",
        "        \n",
        "        # Make prediction\n",
        "        prediction_probs = model.predict(emg_buffer)[0]\n",
        "        prediction = np.argmax(prediction_probs)\n",
        "        confidence = prediction_probs[prediction]\n",
        "        \n",
        "    else:\n",
        "        # Extract features\n",
        "        extractor = FeatureExtractor(window_size=len(emg_buffer), step_size=1)\n",
        "        features = extractor.extract_time_domain_features(emg_buffer)\n",
        "        freq_features = extractor.extract_frequency_domain_features(emg_buffer)\n",
        "        features.update(freq_features)\n",
        "        \n",
        "        # Create feature dataframe\n",
        "        feature_df = pd.DataFrame([features])\n",
        "        \n",
        "        # Select relevant features\n",
        "        feature_df = feature_df[model_data['feature_names']]\n",
        "        \n",
        "        # Scale if necessary\n",
        "        if model_data['scaler'] is not None:\n",
        "            feature_values = model_data['scaler'].transform(feature_df)\n",
        "        else:\n",
        "            feature_values = feature_df.values\n",
        "        \n",
        "        # Make prediction\n",
        "        model = model_data['model']\n",
        "        prediction = model.predict(feature_values)[0]\n",
        "        probability = model.predict_proba(feature_values)[0]\n",
        "        \n",
        "        confidence = probability[prediction]\n",
        "    \n",
        "    # Map prediction to gesture name\n",
        "    gesture_map = {0: 'rest', 1: 'rotate', 2: 'pinch'}\n",
        "    gesture = gesture_map[prediction]\n",
        "    \n",
        "    return gesture, confidence\n",
        "\n",
        "# Test the real-time prediction function\n",
        "test_buffer = df['filtered_value'].values[:50]  # Get a test window\n",
        "if best_model_name == 'CNN-LSTM':\n",
        "    loaded_model_data = {\n",
        "        'model_type': 'CNN-LSTM',\n",
        "        'model_path': 'best_cnn_lstm_model.h5',\n",
        "        'window_size': extractor.window_size,\n",
        "        'step_size': extractor.step_size\n",
        "    }\n",
        "else:\n",
        "    loaded_model_data = joblib.load('emg_gesture_model.pkl')\n",
        "\n",
        "gesture, confidence = predict_gesture_realtime(test_buffer, loaded_model_data)\n",
        "print(f\"\\nTest prediction: {gesture} (confidence: {confidence:.2%})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74c1a6d",
      "metadata": {},
      "source": [
        "## 10. ESP32 Integration Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e4075d",
      "metadata": {},
      "outputs": [],
      "source": [
        "esp32_code = \"\"\"\n",
        "// ESP32-S3 EMG Gesture Recognition\n",
        "// Using Gravity Analog EMG Sensor by OYMotion\n",
        "// Modified for 3-class classification (rest, rotate, pinch)\n",
        "#include <Arduino.h>\n",
        "const int EMG_PIN = 34;  // Analog input pin\n",
        "const int WINDOW_SIZE = 50;\n",
        "const int STEP_SIZE = 10;\n",
        "const int SAMPLING_RATE = 1000;  // Hz\n",
        "float emg_buffer[WINDOW_SIZE];\n",
        "int buffer_index = 0;\n",
        "\n",
        "// Feature extraction functions\n",
        "float calculate_mean(float* data, int len) {\n",
        "    float sum = 0;\n",
        "    for(int i = 0; i < len; i++) {\n",
        "        sum += data[i];\n",
        "    }\n",
        "    return sum / len;\n",
        "}\n",
        "\n",
        "float calculate_std(float* data, int len) {\n",
        "    float mean = calculate_mean(data, len);\n",
        "    float sum = 0;\n",
        "    for(int i = 0; i < len; i++) {\n",
        "        sum += pow(data[i] - mean, 2);\n",
        "    }\n",
        "    return sqrt(sum / len);\n",
        "}\n",
        "\n",
        "float calculate_rms(float* data, int len) {\n",
        "    float sum = 0;\n",
        "    for(int i = 0; i < len; i++) {\n",
        "        sum += pow(data[i], 2);\n",
        "    }\n",
        "    return sqrt(sum / len);\n",
        "}\n",
        "\n",
        "int calculate_zcr(float* data, int len) {\n",
        "    int count = 0;\n",
        "    for(int i = 1; i < len; i++) {\n",
        "        if((data[i] >= 0 && data[i-1] < 0) || \n",
        "           (data[i] < 0 && data[i-1] >= 0)) {\n",
        "            count++;\n",
        "        }\n",
        "    }\n",
        "    return count;\n",
        "}\n",
        "\n",
        "void setup() {\n",
        "    Serial.begin(115200);\n",
        "    pinMode(EMG_PIN, INPUT);\n",
        "}\n",
        "\n",
        "void loop() {\n",
        "    // Read EMG value\n",
        "    int raw_value = analogRead(EMG_PIN);\n",
        "    float emg_value = (raw_value / 4095.0) * 4000;  // Convert to 0-4000 range\n",
        "    \n",
        "    // Add to buffer\n",
        "    emg_buffer[buffer_index] = emg_value;\n",
        "    buffer_index = (buffer_index + 1) % WINDOW_SIZE;\n",
        "    \n",
        "    // Process when buffer is full\n",
        "    if(buffer_index == 0) {\n",
        "        // Extract features\n",
        "        float mean = calculate_mean(emg_buffer, WINDOW_SIZE);\n",
        "        float std = calculate_std(emg_buffer, WINDOW_SIZE);\n",
        "        float rms = calculate_rms(emg_buffer, WINDOW_SIZE);\n",
        "        int zcr = calculate_zcr(emg_buffer, WINDOW_SIZE);\n",
        "        \n",
        "        // Send features to serial for processing\n",
        "        Serial.print(\"FEATURES:\");\n",
        "        Serial.print(mean); Serial.print(\",\");\n",
        "        Serial.print(std); Serial.print(\",\");\n",
        "        Serial.print(rms); Serial.print(\",\");\n",
        "        Serial.print(zcr);\n",
        "        Serial.println();\n",
        "    }\n",
        "    \n",
        "    delay(1000 / SAMPLING_RATE);  // Maintain sampling rate\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ESP32 Integration Code saved in the script\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save ESP32 code to file\n",
        "with open('esp32_emg_gesture.ino', 'w') as f:\n",
        "    f.write(esp32_code)\n",
        "print(\"ESP32 code saved as 'esp32_emg_gesture.ino'\")\n",
        "print(\"\\nComplete EMG Gesture Recognition System Ready for 3-class classification!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Upload the ESP32 code to your microcontroller\")\n",
        "print(\"2. Use the saved model for real-time predictions\")\n",
        "print(\"3. Fine-tune the model with more data if needed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
